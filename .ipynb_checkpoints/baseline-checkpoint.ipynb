{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:57: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "100%|██████████| 3963/3963 [15:48<00:00,  4.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import data\n",
    "import utils\n",
    "import resnet as caffe_resnet\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = caffe_resnet.resnet50(pretrained=True)\n",
    "\n",
    "        def save_output(module, input, output):\n",
    "            self.buffer = output\n",
    "        self.model.layer4.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def create_vqa_loader(*paths):\n",
    "    transform = utils.get_transform(config.image_size, config.central_fraction)\n",
    "    datasets = [data.VSQImages(path, transform=transform) for path in paths]\n",
    "    dataset = data.Composite(*datasets)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.preprocess_batch_size,\n",
    "        num_workers=config.data_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    net = Net().cuda()\n",
    "    net.eval()\n",
    "\n",
    "    loader = create_vqa_loader(config.train_path, config.val_path)\n",
    "    features_shape = (\n",
    "        len(loader.dataset),\n",
    "        config.output_features,\n",
    "        config.output_size,\n",
    "        config.output_size\n",
    "    )\n",
    "\n",
    "    with h5py.File(config.preprocessed_path, libver='latest') as fd:\n",
    "        features = fd.create_dataset('features', shape=features_shape, dtype='float16')\n",
    "        vsq_ids = fd.create_dataset('ids', shape=(len(loader.dataset),), dtype='int32')\n",
    "\n",
    "        i = j = 0\n",
    "        for ids, imgs in tqdm(loader):\n",
    "            imgs = Variable(imgs).cuda(device=None, non_blocking=True)\n",
    "            out = net(imgs)\n",
    "\n",
    "            j = i + imgs.size(0)\n",
    "            features[i:j, :, :] = out.data.cpu().numpy().astype('float16')\n",
    "            vsq_ids[i:j] = ids.numpy().astype('int32')\n",
    "            i = j\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will save to logs/2020-12-11_03-59-53.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ssl/vqa/data.py:113: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  answer_has_index = len(answers.nonzero()) > 0\n",
      "/home/ubuntu/ssl/vqa/models.py:103: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(w)\n",
      "/home/ubuntu/ssl/vqa/models.py:96: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.embedding.weight) #embedding.weight gives the learnable weights of the module of\n",
      "/home/ubuntu/ssl/vqa/models.py:41: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(m.weight)\n",
      "train E000:   0% 0/156 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "train E000: 100% 156/156 [00:50<00:00,  3.07it/s, acc=0.3161, loss=4.2790]\n",
      "val E000: 100% 34/34 [00:10<00:00,  3.15it/s, acc=0.3879, loss=2.8102]\n",
      "train E001: 100% 156/156 [00:49<00:00,  3.15it/s, acc=0.3294, loss=3.4207]\n",
      "val E001: 100% 34/34 [00:10<00:00,  3.09it/s, acc=0.3664, loss=2.6934]\n",
      "train E002: 100% 156/156 [00:48<00:00,  3.19it/s, acc=0.3662, loss=3.3602]\n",
      "val E002: 100% 34/34 [00:10<00:00,  3.18it/s, acc=0.3796, loss=2.5859]\n",
      "train E003: 100% 156/156 [00:49<00:00,  3.18it/s, acc=0.3297, loss=3.2385]\n",
      "val E003: 100% 34/34 [00:10<00:00,  3.12it/s, acc=0.3616, loss=2.5531]\n",
      "train E004: 100% 156/156 [00:49<00:00,  3.15it/s, acc=0.4064, loss=3.0533]\n",
      "val E004: 100% 34/34 [00:10<00:00,  3.14it/s, acc=0.3538, loss=2.4830]\n",
      "train E005:  77% 120/156 [00:39<00:09,  3.72it/s, acc=0.3967, loss=3.0337]"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import math\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import data\n",
    "import models\n",
    "import utils\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "def update_learning_rate(optimizer, iteration):\n",
    "    lr = config.initial_lr * 0.5**(float(iteration) / config.lr_halflife)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "\n",
    "def run(net, loader, optimizer, tracker, train=False, prefix='', epoch=0):\n",
    "    \"\"\" Run an epoch over the given loader \"\"\"\n",
    "    if train:\n",
    "        net.train()\n",
    "        tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "    else:\n",
    "        net.eval()\n",
    "        tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "        answ = []\n",
    "        idxs = []\n",
    "        accs = []\n",
    "\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(prefix), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(prefix), tracker_class(**tracker_params))\n",
    "\n",
    "    log_softmax = nn.LogSoftmax().cuda()\n",
    "    for v, q, a, idx, q_len in tq:\n",
    "        requires_grad = False;\n",
    "        v = Variable(v, requires_grad)\n",
    "        q = Variable(q, requires_grad)\n",
    "        a = Variable(a, requires_grad)\n",
    "        q_len = Variable(q_len, requires_grad)\n",
    "\n",
    "        v = v.cuda()\n",
    "        q = q.cuda()\n",
    "        a = a.cuda()\n",
    "        q_len = q_len.cuda()\n",
    "\n",
    "        out = net(v, q, q_len)\n",
    "        nll = -log_softmax(out)\n",
    "        loss = (nll * a / 10).sum(dim=1).mean()\n",
    "        acc = utils.batch_accuracy(out.data, a.data).cpu()\n",
    "\n",
    "        if train:\n",
    "            global total_iterations\n",
    "            update_learning_rate(optimizer, total_iterations)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_iterations += 1\n",
    "        else:\n",
    "            # store information about evaluation of this minibatch\n",
    "            _, answer = out.data.cpu().max(dim=1)\n",
    "            answ.append(answer.view(-1))\n",
    "            accs.append(acc.view(-1))\n",
    "            idxs.append(idx.view(-1).clone())\n",
    "\n",
    "        loss_tracker.append(loss.data.item())\n",
    "        # acc_tracker.append(acc.mean())\n",
    "        for a in acc:\n",
    "            acc_tracker.append(a.item())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))\n",
    "\n",
    "    if not train:\n",
    "        answ = list(torch.cat(answ, dim=0))\n",
    "        accs = list(torch.cat(accs, dim=0))\n",
    "        idxs = list(torch.cat(idxs, dim=0))\n",
    "        return answ, accs, idxs\n",
    "\n",
    "\n",
    "def main():\n",
    "    from datetime import datetime\n",
    "\n",
    "    # this has been changed to run jupyter\n",
    "    #\n",
    "    # non jupyter ##############################################################\n",
    "    if len(sys.argv) > 1:\n",
    "        name = ' '.join(sys.argv[1:])\n",
    "    else:\n",
    "        name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    # remove line below if not running on jupyter\n",
    "    name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    target_name = os.path.join('logs', '{}.pth'.format(name))\n",
    "    print('will save to {}'.format(target_name))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    train_loader = data.get_loader(train=True)\n",
    "    val_loader = data.get_loader(val=True)\n",
    "\n",
    "    net = nn.DataParallel(models.Net(train_loader.dataset.num_tokens)).cuda() #change made here\n",
    "    optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad])\n",
    "\n",
    "    tracker = utils.Tracker()\n",
    "    config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
    "\n",
    "    for i in range(config.epochs):\n",
    "        _ = run(net, train_loader, optimizer, tracker, train=True, prefix='train', epoch=i)\n",
    "        r = run(net, val_loader, optimizer, tracker, train=False, prefix='val', epoch=i)\n",
    "\n",
    "        results = {\n",
    "            'name': name,\n",
    "            'tracker': tracker.to_dict(),\n",
    "            'config': config_as_dict,\n",
    "            'weights': net.state_dict(),\n",
    "            'eval': {\n",
    "                'answers': r[0],\n",
    "                'accuracies': r[1],\n",
    "                'idx': r[2],\n",
    "            },\n",
    "            'vocab': train_loader.dataset.vocab,\n",
    "        }\n",
    "        torch.save(results, target_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
