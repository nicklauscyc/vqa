{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:57: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "100%|██████████| 3963/3963 [15:51<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import data\n",
    "import utils\n",
    "import resnet as caffe_resnet\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.model = caffe_resnet.resnet18(pretrained=False)\n",
    "\n",
    "        def save_output(module, input, output):\n",
    "            self.buffer = output\n",
    "        self.model.layer4.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.model(x)\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "def create_vqa_loader(*paths):\n",
    "    transform = utils.get_transform(config.image_size, config.central_fraction)\n",
    "    datasets = [data.VSQImages(path, transform=transform) for path in paths]\n",
    "    dataset = data.Composite(*datasets)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.preprocess_batch_size,\n",
    "        num_workers=config.data_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "def main():\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    net = Net().cuda()\n",
    "    net.eval()\n",
    "\n",
    "    loader = create_vqa_loader(config.train_path, config.val_path)\n",
    "    features_shape = (\n",
    "        len(loader.dataset),\n",
    "        config.output_features,\n",
    "        config.output_size,\n",
    "        config.output_size\n",
    "    )\n",
    "\n",
    "    with h5py.File(config.preprocessed_path, libver='latest') as fd:\n",
    "        features = fd.create_dataset('features', shape=features_shape, dtype='float16')\n",
    "        vsq_ids = fd.create_dataset('ids', shape=(len(loader.dataset),), dtype='int32')\n",
    "\n",
    "        i = j = 0\n",
    "        for ids, imgs in tqdm(loader):\n",
    "            imgs = Variable(imgs).cuda(device=None, non_blocking=True)\n",
    "            out = net(imgs)\n",
    "\n",
    "            j = i + imgs.size(0)\n",
    "            features[i:j, :, :] = out.data.cpu().numpy().astype('float16')\n",
    "            vsq_ids[i:j] = ids.numpy().astype('int32')\n",
    "            i = j\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json_access \n",
    "from collections import Counter\n",
    "import itertools\n",
    "from data import *\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import config\n",
    "\n",
    "#adapted from: https://github.com/Cyanogenoid/pytorch-vqa/blob/master/preprocess-vocab.py\n",
    "\n",
    "#split = 'train'\n",
    "#annFile='Annotations/%s.json'%(split)\n",
    "#imgDir = 'train'\n",
    "\n",
    "# initialize VQA api for QA annotations\n",
    "#vqa=json_access.VQA(annFile)\n",
    "\n",
    "#imgs = vqa.getImgs()\n",
    "#anns = vqa.getAnns(imgs=imgs)\n",
    "\n",
    "def extract_vocab(iterable, top_k=None, start=0):\n",
    "    \"\"\" Turns an iterable of list of tokens into a vocabulary.\n",
    "        These tokens could be single answers or word tokens in questions.\n",
    "    \"\"\"\n",
    "    all_tokens = itertools.chain.from_iterable(iterable)\n",
    "    counter = Counter(all_tokens)\n",
    "    if top_k:\n",
    "        most_common = counter.most_common(top_k)\n",
    "        most_common = (t for t, c in most_common)\n",
    "    else:\n",
    "        most_common = counter.keys()\n",
    "    # descending in count, then lexicographical order\n",
    "    tokens = sorted(most_common, key=lambda x: (counter[x], x), reverse=True)\n",
    "    vocab = {t: i for i, t in enumerate(tokens, start=start)}\n",
    "    return vocab\n",
    "\n",
    "def main():\n",
    "    questions = utils.path_for(train=True, question=True)\n",
    "    answers = utils.path_for(train=True, answer=True)\n",
    "\n",
    "    with open(questions, 'r') as fd:\n",
    "        questions = json.load(fd)\n",
    "    with open(answers, 'r') as fd:\n",
    "        answers = json.load(fd)\n",
    "\n",
    "    questions = list(prepare_questions(questions))\n",
    "    answers = list(prepare_answers(answers))\n",
    "    question_vocab = extract_vocab(questions, start=1)\n",
    "    answer_vocab = extract_vocab(answers, top_k=config.max_answers) #what should top_k be here?\n",
    "    vocabs = {\n",
    "        'question': question_vocab,\n",
    "        'answer': answer_vocab,\n",
    "    }\n",
    "    with open(config.vocabulary_path, 'w') as fd:\n",
    "        json.dump(vocabs, fd)\n",
    "    \n",
    "    #v = list(encode_answers(a, answer_vocab) for a in answers)\n",
    "    #print(v)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will save to logs/2020-12-12_21-40-23.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train E000:   0% 0/156 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/ipykernel/__main__.py:60: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "train E000: 100% 156/156 [00:44<00:00,  3.52it/s, acc=0.3275, loss=4.2500]\n",
      "val E000: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3635, loss=2.8808]\n",
      "train E001: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.3542, loss=3.5857]\n",
      "val E001: 100% 34/34 [00:09<00:00,  3.53it/s, acc=0.3803, loss=2.7769]\n",
      "train E002: 100% 156/156 [00:45<00:00,  3.43it/s, acc=0.4161, loss=3.4322]\n",
      "val E002: 100% 34/34 [00:09<00:00,  3.57it/s, acc=0.3656, loss=2.7215]\n",
      "train E003: 100% 156/156 [00:45<00:00,  3.44it/s, acc=0.2874, loss=3.4230]\n",
      "val E003: 100% 34/34 [00:10<00:00,  3.37it/s, acc=0.3407, loss=2.7068]\n",
      "train E004: 100% 156/156 [00:47<00:00,  3.30it/s, acc=0.3575, loss=3.2042]\n",
      "val E004: 100% 34/34 [00:09<00:00,  3.51it/s, acc=0.3432, loss=2.6893]\n",
      "train E005: 100% 156/156 [00:45<00:00,  3.44it/s, acc=0.2804, loss=3.1921]\n",
      "val E005: 100% 34/34 [00:09<00:00,  3.57it/s, acc=0.3331, loss=2.6408]\n",
      "train E006: 100% 156/156 [00:44<00:00,  3.53it/s, acc=0.3473, loss=3.1501]\n",
      "val E006: 100% 34/34 [00:09<00:00,  3.54it/s, acc=0.3162, loss=2.6505]\n",
      "train E007: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.3535, loss=3.1269]\n",
      "val E007: 100% 34/34 [00:09<00:00,  3.50it/s, acc=0.3260, loss=2.6036]\n",
      "train E008: 100% 156/156 [00:45<00:00,  3.41it/s, acc=0.3612, loss=3.0302]\n",
      "val E008: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3317, loss=2.6198]\n",
      "train E009: 100% 156/156 [00:44<00:00,  3.53it/s, acc=0.3739, loss=2.9591]\n",
      "val E009: 100% 34/34 [00:09<00:00,  3.43it/s, acc=0.3428, loss=2.6075]\n",
      "train E010: 100% 156/156 [00:45<00:00,  3.46it/s, acc=0.3952, loss=2.9418]\n",
      "val E010: 100% 34/34 [00:09<00:00,  3.52it/s, acc=0.3238, loss=2.5957]\n",
      "train E011: 100% 156/156 [00:45<00:00,  3.42it/s, acc=0.3604, loss=2.8725]\n",
      "val E011: 100% 34/34 [00:09<00:00,  3.53it/s, acc=0.3209, loss=2.5841]\n",
      "train E012: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.3986, loss=2.8496]\n",
      "val E012: 100% 34/34 [00:09<00:00,  3.53it/s, acc=0.3366, loss=2.5895]\n",
      "train E013: 100% 156/156 [00:44<00:00,  3.54it/s, acc=0.3412, loss=2.7518]\n",
      "val E013: 100% 34/34 [00:09<00:00,  3.54it/s, acc=0.3363, loss=2.6043]\n",
      "train E014: 100% 156/156 [00:43<00:00,  3.55it/s, acc=0.4242, loss=2.7208]\n",
      "val E014: 100% 34/34 [00:09<00:00,  3.61it/s, acc=0.3266, loss=2.6036]\n",
      "train E015: 100% 156/156 [00:45<00:00,  3.45it/s, acc=0.3684, loss=2.6778]\n",
      "val E015: 100% 34/34 [00:09<00:00,  3.58it/s, acc=0.3317, loss=2.6059]\n",
      "train E016: 100% 156/156 [00:45<00:00,  3.44it/s, acc=0.4249, loss=2.7181]\n",
      "val E016: 100% 34/34 [00:09<00:00,  3.54it/s, acc=0.3358, loss=2.6217]\n",
      "train E017: 100% 156/156 [00:44<00:00,  3.52it/s, acc=0.3172, loss=2.6630]\n",
      "val E017: 100% 34/34 [00:09<00:00,  3.50it/s, acc=0.3239, loss=2.6384]\n",
      "train E018: 100% 156/156 [00:46<00:00,  3.38it/s, acc=0.4214, loss=2.5759]\n",
      "val E018: 100% 34/34 [00:09<00:00,  3.56it/s, acc=0.3364, loss=2.6123]\n",
      "train E019: 100% 156/156 [00:44<00:00,  3.49it/s, acc=0.3868, loss=2.5488]\n",
      "val E019: 100% 34/34 [00:09<00:00,  3.50it/s, acc=0.3334, loss=2.6479]\n",
      "train E020: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.4144, loss=2.4569]\n",
      "val E020: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3390, loss=2.6773]\n",
      "train E021: 100% 156/156 [00:45<00:00,  3.45it/s, acc=0.3554, loss=2.4968]\n",
      "val E021: 100% 34/34 [00:09<00:00,  3.50it/s, acc=0.3253, loss=2.6879]\n",
      "train E022: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.4521, loss=2.4824]\n",
      "val E022: 100% 34/34 [00:09<00:00,  3.57it/s, acc=0.3339, loss=2.6646]\n",
      "train E023: 100% 156/156 [00:44<00:00,  3.49it/s, acc=0.4697, loss=2.4038]\n",
      "val E023: 100% 34/34 [00:09<00:00,  3.45it/s, acc=0.3318, loss=2.7170]\n",
      "train E024: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.3939, loss=2.4716]\n",
      "val E024: 100% 34/34 [00:09<00:00,  3.53it/s, acc=0.3353, loss=2.7145]\n",
      "train E025: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.3731, loss=2.4312]\n",
      "val E025: 100% 34/34 [00:09<00:00,  3.56it/s, acc=0.3248, loss=2.6959]\n",
      "train E026: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.4035, loss=2.4003]\n",
      "val E026: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3203, loss=2.7354]\n",
      "train E027: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.4258, loss=2.3334]\n",
      "val E027: 100% 34/34 [00:09<00:00,  3.59it/s, acc=0.3144, loss=2.7149]\n",
      "train E028: 100% 156/156 [00:45<00:00,  3.46it/s, acc=0.3750, loss=2.3867]\n",
      "val E028: 100% 34/34 [00:09<00:00,  3.51it/s, acc=0.3277, loss=2.7405]\n",
      "train E029: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.4323, loss=2.3297]\n",
      "val E029: 100% 34/34 [00:09<00:00,  3.56it/s, acc=0.3177, loss=2.7544]\n",
      "train E030: 100% 156/156 [00:44<00:00,  3.52it/s, acc=0.4172, loss=2.3429]\n",
      "val E030: 100% 34/34 [00:09<00:00,  3.47it/s, acc=0.3308, loss=2.7658]\n",
      "train E031: 100% 156/156 [00:45<00:00,  3.44it/s, acc=0.3739, loss=2.2724]\n",
      "val E031: 100% 34/34 [00:09<00:00,  3.54it/s, acc=0.3109, loss=2.7602]\n",
      "train E032: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.4560, loss=2.2721]\n",
      "val E032: 100% 34/34 [00:09<00:00,  3.52it/s, acc=0.3093, loss=2.8187]\n",
      "train E033: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.4167, loss=2.2586]\n",
      "val E033: 100% 34/34 [00:09<00:00,  3.57it/s, acc=0.3113, loss=2.7822]\n",
      "train E034: 100% 156/156 [00:44<00:00,  3.48it/s, acc=0.4240, loss=2.2351]\n",
      "val E034: 100% 34/34 [00:09<00:00,  3.56it/s, acc=0.3139, loss=2.8019]\n",
      "train E035: 100% 156/156 [00:44<00:00,  3.53it/s, acc=0.3521, loss=2.2112]\n",
      "val E035: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3363, loss=2.7939]\n",
      "train E036: 100% 156/156 [00:44<00:00,  3.52it/s, acc=0.4255, loss=2.2579]\n",
      "val E036: 100% 34/34 [00:09<00:00,  3.50it/s, acc=0.3294, loss=2.8102]\n",
      "train E037: 100% 156/156 [00:44<00:00,  3.53it/s, acc=0.4109, loss=2.2348]\n",
      "val E037: 100% 34/34 [00:10<00:00,  3.34it/s, acc=0.3182, loss=2.8496]\n",
      "train E038: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.4171, loss=2.2098]\n",
      "val E038: 100% 34/34 [00:09<00:00,  3.58it/s, acc=0.3199, loss=2.8386]\n",
      "train E039: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.5299, loss=2.2291]\n",
      "val E039: 100% 34/34 [00:09<00:00,  3.54it/s, acc=0.3348, loss=2.8550]\n",
      "train E040: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.4227, loss=2.2081]\n",
      "val E040: 100% 34/34 [00:09<00:00,  3.60it/s, acc=0.3264, loss=2.8714]\n",
      "train E041: 100% 156/156 [00:45<00:00,  3.41it/s, acc=0.4179, loss=2.1798]\n",
      "val E041: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3288, loss=2.8620]\n",
      "train E042: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.3875, loss=2.1478]\n",
      "val E042: 100% 34/34 [00:09<00:00,  3.48it/s, acc=0.3252, loss=2.8871]\n",
      "train E043: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.4563, loss=2.1309]\n",
      "val E043: 100% 34/34 [00:09<00:00,  3.59it/s, acc=0.3162, loss=2.8953]\n",
      "train E044: 100% 156/156 [00:45<00:00,  3.44it/s, acc=0.4439, loss=2.1438]\n",
      "val E044: 100% 34/34 [00:09<00:00,  3.60it/s, acc=0.3214, loss=2.9097]\n",
      "train E045: 100% 156/156 [00:44<00:00,  3.49it/s, acc=0.3840, loss=2.1846]\n",
      "val E045: 100% 34/34 [00:09<00:00,  3.56it/s, acc=0.3253, loss=2.9250]\n",
      "train E046: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.4022, loss=2.1789]\n",
      "val E046: 100% 34/34 [00:09<00:00,  3.55it/s, acc=0.3205, loss=2.8937]\n",
      "train E047: 100% 156/156 [00:45<00:00,  3.41it/s, acc=0.4631, loss=2.1375]\n",
      "val E047: 100% 34/34 [00:09<00:00,  3.47it/s, acc=0.3139, loss=2.9415]\n",
      "train E048: 100% 156/156 [00:44<00:00,  3.51it/s, acc=0.4067, loss=2.1338]\n",
      "val E048: 100% 34/34 [00:09<00:00,  3.50it/s, acc=0.3146, loss=2.9693]\n",
      "train E049: 100% 156/156 [00:44<00:00,  3.50it/s, acc=0.3845, loss=2.1245]\n",
      "val E049: 100% 34/34 [00:09<00:00,  3.54it/s, acc=0.3222, loss=2.9517]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os.path\n",
    "import math\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import config\n",
    "import data\n",
    "import modelNoAttention\n",
    "import utils\n",
    "\n",
    "import h5py\n",
    "\n",
    "\n",
    "def update_learning_rate(optimizer, iteration):\n",
    "    lr = config.initial_lr * 0.5**(float(iteration) / config.lr_halflife)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "total_iterations = 0\n",
    "\n",
    "\n",
    "def run(net, loader, optimizer, tracker, train=False, prefix='', epoch=0):\n",
    "    \"\"\" Run an epoch over the given loader \"\"\"\n",
    "    if train:\n",
    "        net.train()\n",
    "        tracker_class, tracker_params = tracker.MovingMeanMonitor, {'momentum': 0.99}\n",
    "    else:\n",
    "        net.eval()\n",
    "        tracker_class, tracker_params = tracker.MeanMonitor, {}\n",
    "        answ = []\n",
    "        idxs = []\n",
    "        accs = []\n",
    "\n",
    "    tq = tqdm(loader, desc='{} E{:03d}'.format(prefix, epoch), ncols=0)\n",
    "    loss_tracker = tracker.track('{}_loss'.format(prefix), tracker_class(**tracker_params))\n",
    "    acc_tracker = tracker.track('{}_acc'.format(prefix), tracker_class(**tracker_params))\n",
    "\n",
    "    log_softmax = nn.LogSoftmax().cuda()\n",
    "    for v, q, a, idx, q_len in tq:\n",
    "        requires_grad = False;\n",
    "        v = Variable(v, requires_grad)\n",
    "        q = Variable(q, requires_grad)\n",
    "        a = Variable(a, requires_grad)\n",
    "        q_len = Variable(q_len, requires_grad)\n",
    "\n",
    "        v = v.cuda()\n",
    "        q = q.cuda()\n",
    "        a = a.cuda()\n",
    "        q_len = q_len.cuda()\n",
    "\n",
    "        out = net(v, q, q_len)\n",
    "        nll = -log_softmax(out)\n",
    "        loss = (nll * a / 10).sum(dim=1).mean()\n",
    "        acc = utils.batch_accuracy(out.data, a.data).cpu()\n",
    "\n",
    "        if train:\n",
    "            global total_iterations\n",
    "            update_learning_rate(optimizer, total_iterations)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_iterations += 1\n",
    "        else:\n",
    "            # store information about evaluation of this minibatch\n",
    "            _, answer = out.data.cpu().max(dim=1)\n",
    "            answ.append(answer.view(-1))\n",
    "            accs.append(acc.view(-1))\n",
    "            idxs.append(idx.view(-1).clone())\n",
    "\n",
    "        loss_tracker.append(loss.data.item())\n",
    "        # acc_tracker.append(acc.mean())\n",
    "        for a in acc:\n",
    "            acc_tracker.append(a.item())\n",
    "        fmt = '{:.4f}'.format\n",
    "        tq.set_postfix(loss=fmt(loss_tracker.mean.value), acc=fmt(acc_tracker.mean.value))\n",
    "\n",
    "    if not train:\n",
    "        answ = list(torch.cat(answ, dim=0))\n",
    "        accs = list(torch.cat(accs, dim=0))\n",
    "        idxs = list(torch.cat(idxs, dim=0))\n",
    "        return answ, accs, idxs\n",
    "\n",
    "\n",
    "def main():\n",
    "    from datetime import datetime\n",
    "\n",
    "    # this has been changed to run jupyter\n",
    "    #\n",
    "    # non jupyter ##############################################################\n",
    "    if len(sys.argv) > 1:\n",
    "        name = ' '.join(sys.argv[1:])\n",
    "    else:\n",
    "        name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    ############################################################################\n",
    "\n",
    "\n",
    "    # remove line below if not running on jupyter\n",
    "    name = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    target_name = os.path.join('logs', '{}.pth'.format(name))\n",
    "    print('will save to {}'.format(target_name))\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    train_loader = data.get_loader(train=True)\n",
    "    val_loader = data.get_loader(val=True)\n",
    "\n",
    "    net = nn.DataParallel(models.Net(train_loader.dataset.num_tokens)).cuda() #change made here\n",
    "    optimizer = optim.Adam([p for p in net.parameters() if p.requires_grad])\n",
    "\n",
    "    tracker = utils.Tracker()\n",
    "    config_as_dict = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
    "\n",
    "    for i in range(config.epochs):\n",
    "        _ = run(net, train_loader, optimizer, tracker, train=True, prefix='train', epoch=i)\n",
    "        r = run(net, val_loader, optimizer, tracker, train=False, prefix='val', epoch=i)\n",
    "\n",
    "        results = {\n",
    "            'name': name,\n",
    "            'tracker': tracker.to_dict(),\n",
    "            'config': config_as_dict,\n",
    "            'weights': net.state_dict(),\n",
    "            'eval': {\n",
    "                'answers': r[0],\n",
    "                'accuracies': r[1],\n",
    "                'idx': r[2],\n",
    "            },\n",
    "            'vocab': train_loader.dataset.vocab,\n",
    "        }\n",
    "        torch.save(results, target_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_latest_p37)",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
